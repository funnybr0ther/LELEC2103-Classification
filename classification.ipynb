{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker\n",
    "import torchaudio\n",
    "\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import sounddevice as sd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"chirping_birds\", \"crackling_fire\", \"handsaw\", \"chainsaw\", \"helicopter\"]\n",
    "audio_path = './ESC-50/audio/'\n",
    "meta_path = './ESC-50/meta/'\n",
    "\n",
    "\n",
    "data = pd.read_csv(meta_path + 'esc50.csv')\n",
    "\n",
    "## Remove rows where the category does not belong to categories\n",
    "data = data[data.category.isin(categories)]\n",
    "\n",
    "re_encoder = OrdinalEncoder(dtype=np.long)\n",
    "re_encoder.fit(data[[\"target\"]])\n",
    "data[[\"target\"]] = re_encoder.transform(data[[\"target\"]])\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioUtil:\n",
    "    @staticmethod\n",
    "    def open(path):\n",
    "        return torchaudio.load(path)\n",
    "          \n",
    "    @staticmethod\n",
    "    def toFile(aud,path):\n",
    "        sig, sr = aud\n",
    "        torchaudio.save(path,sig,sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def displayTime(aud):\n",
    "        sig, sr = aud\n",
    "        num_channels,num_samples = sig.shape\n",
    "\n",
    "        if(num_channels != 1):\n",
    "            raise Exception(\"Can't display multi-channel sound.\")\n",
    "\n",
    "        t = np.linspace(0,num_samples/sr,num_samples)\n",
    "        plt.plot(t,sig.numpy().ravel())\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def playSound(aud):\n",
    "        sig, sr = aud\n",
    "        sd.play(0.1*sig.numpy().ravel(),sr,blocking=True)\n",
    "\n",
    "    @staticmethod\n",
    "    def toMelSpec(aud,n_fft = 512, n_mels = 20, truncate_to=0):\n",
    "        sig,sr=aud\n",
    "        num_channels,num_samples = sig.shape\n",
    "        if(truncate_to>0):\n",
    "            sig = sig[:,:truncate_to*n_fft]\n",
    "        \n",
    "        melspec = transforms.MelSpectrogram(sample_rate = sr,n_fft=n_fft,hop_length=n_fft,n_mels=n_mels,center=False)(sig)\n",
    "        melspec = transforms.AmplitudeToDB()(melspec)\n",
    "        return melspec\n",
    "    \n",
    "    @staticmethod\n",
    "    def displayMelspec(melspec, aud):\n",
    "        sig,sr = aud\n",
    "        num_channels,num_samples = sig.shape\n",
    "\n",
    "        tmax = num_samples/sr\n",
    "\n",
    "        t = np.linspace(0,tmax,5)\n",
    "        \n",
    "        plt.imshow(melspec.numpy()[0], interpolation='nearest', origin=\"lower\",aspect=\"auto\")\n",
    "        plt.colorbar()\n",
    "        xmin,xmax = plt.xlim()\n",
    "        plt.xticks(t*xmax/tmax,[\"{:10.4f}\".format(i) for i in t])\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def rechannel(aud):\n",
    "        sig, sr = aud\n",
    "        \n",
    "        num_channels, num_samples  = sig.shape\n",
    "\n",
    "        if(num_channels == 1):\n",
    "            return aud\n",
    "        \n",
    "        else:\n",
    "            ## Average the channels\n",
    "            sig = sig.mean(axis = 1)\n",
    "            return (sig, sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def resample(aud, new_sr):\n",
    "        sig, sr = aud\n",
    "\n",
    "        num_channels, num_samples  = sig.shape\n",
    "        if(num_channels != 1):\n",
    "            raise Exception(\"Can't apply resample to multi-channel sound.\")\n",
    "        \n",
    "        resig = torchaudio.transforms.Resample(sr,new_sr)(sig)\n",
    "    \n",
    "        return ((resig,new_sr))\n",
    "        \n",
    "    @staticmethod\n",
    "    def sliceAudio(aud, trunc, index, trunc_type = \"ms\",bootstrap = False,bootstrapSeed=0):\n",
    "        sig, sr = aud\n",
    "        num_channels, num_samples  = sig.shape\n",
    "        \n",
    "    \n",
    "        if(num_channels != 1):\n",
    "            raise Exception(\"Can't apply slice to multi-channel sound.\")\n",
    "\n",
    "        if not bootstrap:\n",
    "            if trunc_type == \"ms\":\n",
    "                samples_per_slice = int(trunc/1000 * sr)\n",
    "                if samples_per_slice*index > num_samples:\n",
    "                    raise Exception(\"Can't extract a slice with index {} from this audio file.\".format(index)) \n",
    "                \n",
    "            elif trunc_type == \"samples\":\n",
    "                samples_per_slice = trunc\n",
    "                if samples_per_slice*index > num_samples:\n",
    "                    raise Exception(\"Can't extract a slice with index {} from this audio file.\".format(index))\n",
    "            else:\n",
    "                raise Exception('''Unknown truncation type, use \"ms\" or \"samples\"''')\n",
    "\n",
    "            return (sig[:,index*samples_per_slice : (index+1)*samples_per_slice],sr)\n",
    "        else:\n",
    "            if trunc_type == \"ms\":\n",
    "                samples_per_slice = int(trunc/1000 * sr)\n",
    "                if samples_per_slice > num_samples:\n",
    "                    raise Exception(\"Can't extract slices of such length from this audio file.\") \n",
    "            elif trunc_type == \"samples\":\n",
    "                samples_per_slice = trunc\n",
    "                if samples_per_slice > num_samples:\n",
    "                    raise Exception(\"Can't extract slices of such length from this audio file.\")\n",
    "            else:\n",
    "                raise Exception('''Unknown truncation type, use \"ms\" or \"samples\"''')\n",
    "        \n",
    "            randomPermut = np.random.RandomState(bootstrapSeed).randint(0,num_samples-samples_per_slice-1,index+1)[index]\n",
    "            return ((sig[:,randomPermut : randomPermut+samples_per_slice],sr))\n",
    "        \n",
    "    @staticmethod\n",
    "    def augment_timeShift(aud,shift_limit,seed):\n",
    "        sig,sr = aud\n",
    "        _, sig_len = sig.shape\n",
    "        shift_amt = int(np.random.RandomState(seed).random(size=1) * shift_limit * sig_len)\n",
    "        return (sig.roll(shift_amt), sr)\n",
    "\n",
    "    @staticmethod\n",
    "    def augment_spectralMask(spec,max_mask_pct,n_freq_masks=1,n_time_masks=1):\n",
    "        \"\"\"\n",
    "        TODO: Don't use for now, must use a seed to work properly\n",
    "        \"\"\"\n",
    "        _, n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        aug_spec = spec\n",
    "\n",
    "        freq_mask_param = max_mask_pct * n_mels\n",
    "\n",
    "        for _ in range(n_freq_masks):\n",
    "            aug_spec = transforms.FrequencyMasking(freq_mask_param)(aug_spec,mask_value)\n",
    "        \n",
    "        time_mask_param = max_mask_pct * n_steps\n",
    "        for _ in range(n_time_masks):\n",
    "            aug_spec = transforms.TimeMasking(time_mask_param)(aug_spec,mask_value)\n",
    "        \n",
    "        return aug_spec\n",
    "\n",
    "    @staticmethod\n",
    "    def augment_timeNoise(aud,noise_power):\n",
    "        \"\"\"\n",
    "        TODO: Use a seed????\n",
    "        \"\"\"\n",
    "        sig,sr = aud\n",
    "        num_channels, num_samples  = sig.shape\n",
    "\n",
    "        sig_aug = sig + np.random.normal(0,scale=noise_power**0.5,size=num_samples).astype('f')\n",
    "\n",
    "        return (sig_aug,sr)\n",
    "        \n",
    "    @staticmethod\n",
    "    def augment_timeLowPass(aud,cutoff_freq):\n",
    "        \"\"\"\n",
    "        Highly unstable at high cutoff frequencies!!!! (>sr/2) TODO: Fix this\n",
    "        \"\"\"\n",
    "        sig,sr = aud\n",
    "        sig_aug = torchaudio.functional.lowpass_biquad(sig,sr,cutoff_freq=cutoff_freq)\n",
    "\n",
    "        return (sig_aug,sr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDS(Dataset):\n",
    "    def __init__(self,df, data_path, bootstrap, bootstrapSeed = np.random.randint(0,20)):\n",
    "        self.df = df\n",
    "        self.data_path = str(data_path)\n",
    "        self.samplesPerFile= 10\n",
    "        self.windowSize = 512\n",
    "        self.duration = self.windowSize*self.samplesPerFile\n",
    "        self.sr = 11025\n",
    "        self.channel = 1\n",
    "        self.bootstrap = bootstrap\n",
    "        self.augmentations = [\"timeShift\", \"noise\", \"lowPass\", \"mask\"]\n",
    "        self.bootstrapSeed = bootstrapSeed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)*self.samplesPerFile\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        index = idx // self.samplesPerFile\n",
    "        loaded_file = self.getAudio(idx)\n",
    "\n",
    "        melSpecGram = AudioUtil.toMelSpec(loaded_file,self.windowSize,20,0)\n",
    "\n",
    "        # if \"mask\" in self.augmentations:\n",
    "            # melSpecGram = AudioUtil.augment_spectralMask(melSpecGram,0.1,1,1)\n",
    "\n",
    "        class_id = self.df[\"target\"].iloc[index]\n",
    "\n",
    "        return melSpecGram,class_id\n",
    "\n",
    "    def getAudio(self,idx):\n",
    "        index = idx // self.samplesPerFile\n",
    "        subindex = idx % self.samplesPerFile\n",
    "\n",
    "        audio_file = self.df[\"filename\"].iloc[index]\n",
    "        loaded_file = AudioUtil.open(self.data_path + audio_file)\n",
    "        loaded_file = AudioUtil.rechannel(loaded_file)\n",
    "        loaded_file = AudioUtil.resample(loaded_file,11025)\n",
    "        loaded_file = AudioUtil.sliceAudio(loaded_file,self.duration,subindex,\"samples\",self.bootstrap,self.bootstrapSeed)\n",
    "        if \"timeShift\" in self.augmentations:\n",
    "            loaded_file = AudioUtil.augment_timeShift(loaded_file,1.0,idx*self.bootstrapSeed)\n",
    "        if \"noise\" in self.augmentations:\n",
    "            loaded_file = AudioUtil.augment_timeNoise(loaded_file,0.0000)\n",
    "        if \"lowPass\" in self.augmentations:\n",
    "            loaded_file = AudioUtil.augment_timeLowPass(loaded_file,5000)\n",
    "        \n",
    "        return loaded_file\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = SoundDS(data,audio_path,True,4)\n",
    "\n",
    "# Random split of 80:20 between training and validation\n",
    "num_items = len(ds)\n",
    "num_train = round(num_items * 0.8)\n",
    "num_val = num_items - num_train\n",
    "train_ds, val_ds = random_split(ds, [num_train, num_val])\n",
    "\n",
    "# Create training and validation data loaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torch.nn as nn\n",
    "\n",
    "# ----------------------------\n",
    "# Audio Classification Model\n",
    "# ----------------------------\n",
    "class AudioClassifier (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=10)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x\n",
    "\n",
    "# Create the model and put it on the GPU if available\n",
    "myModel = AudioClassifier()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "myModel = myModel.to(device)\n",
    "# Check that it is on Cuda\n",
    "next(myModel.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Training Loop\n",
    "# ----------------------------\n",
    "def training(model, train_dl, num_epochs):\n",
    "  # Loss Function, Optimizer and Scheduler\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n",
    "                                                steps_per_epoch=int(len(train_dl)),\n",
    "                                                epochs=num_epochs,\n",
    "                                                anneal_strategy='linear')\n",
    "\n",
    "  # Repeat for each epoch\n",
    "  for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_prediction = 0\n",
    "    total_prediction = 0\n",
    "\n",
    "    # Repeat for each batch in the training set\n",
    "    for i, data in enumerate(train_dl):\n",
    "        # Get the input features and target labels, and put them on the GPU\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Normalize the inputs\n",
    "        inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "        inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Keep stats for Loss and Accuracy\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Get the predicted class with the highest score\n",
    "        _, prediction = torch.max(outputs,1)\n",
    "        # Count of predictions that matched the target label\n",
    "        correct_prediction += (prediction == labels).sum().item()\n",
    "        total_prediction += prediction.shape[0]\n",
    "\n",
    "        #if i % 10 == 0:    # print every 10 mini-batches\n",
    "        #    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "    \n",
    "    # Print stats at the end of the epoch\n",
    "    num_batches = len(train_dl)\n",
    "    avg_loss = running_loss / num_batches\n",
    "    acc = correct_prediction/total_prediction\n",
    "    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n",
    "\n",
    "  print('Finished Training')\n",
    "  \n",
    "num_epochs=10   # Just for demo, adjust this higher.\n",
    "training(myModel, train_dl, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# Inference\n",
    "# ----------------------------\n",
    "def inference (model, val_dl):\n",
    "  correct_prediction = 0\n",
    "  total_prediction = 0\n",
    "\n",
    "  # Disable gradient updates\n",
    "  with torch.no_grad():\n",
    "    for data in val_dl:\n",
    "      # Get the input features and target labels, and put them on the GPU\n",
    "      inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "      # Normalize the inputs\n",
    "      inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "      inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "      # Get predictions\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      # Get the predicted class with the highest score\n",
    "      _, prediction = torch.max(outputs,1)\n",
    "      # Count of predictions that matched the target label\n",
    "      correct_prediction += (prediction == labels).sum().item()\n",
    "      total_prediction += prediction.shape[0]\n",
    "    \n",
    "  acc = correct_prediction/total_prediction\n",
    "  print(f'Accuracy: {acc:.2f}, Total items: {total_prediction}')\n",
    "\n",
    "# Run inference on trained model with the validation set\n",
    "inference(myModel, val_dl)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
